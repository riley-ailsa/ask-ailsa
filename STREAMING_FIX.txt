# CRITICAL FIX for Streaming JSON Issue
# The problem: SYSTEM_PROMPT tells GPT to return JSON, but streaming needs plain markdown

## ROOT CAUSE

The `/chat/stream` endpoint uses the same SYSTEM_PROMPT as the non-streaming endpoint.
That prompt says: "Return a JSON object with answer_markdown and recommended_grants"

But for streaming, we need GPT to output PLAIN MARKDOWN that displays nicely as it streams.

## THE FIX

In `src/api/server.py`, find the `/chat/stream` endpoint (around line 420).

Replace this section:

```python
# Step 3: Stream GPT response
messages = [
    {"role": "system", "content": SYSTEM_PROMPT},  # <-- WRONG! This asks for JSON
    {
        "role": "user",
        "content": (
            build_user_prompt(query, grants)
            + "\n\nContext from grant documents:\n"
            + context
        ),
    },
]
```

With this:

```python
# Step 3: Stream GPT response
# Use a STREAMING-SPECIFIC prompt that outputs plain markdown
STREAMING_SYSTEM_PROMPT = """You are Ailsa, a UK research funding advisor for NIHR and Innovate UK grants.

RESPONSE REQUIREMENTS:
- Output PLAIN MARKDOWN only (not JSON!)
- Be concise: 300-500 words maximum
- Start with 1-2 sentence summary
- Use ## for section headers
- Use **bold** for emphasis
- Use bullet lists for details

YOUR JOB:
1. Brief summary (1-2 sentences)
2. Top 2-3 relevant grants with why they fit
3. Next steps (2-3 actions)

CRITICAL: Output pure markdown text that will be displayed directly to users.
Do NOT wrap your response in JSON. Just write markdown.

The grants shown to users:
""" + "\n".join([f"- {g['title']} ({g['source']})" for g in grants[:5]])

messages = [
    {"role": "system", "content": STREAMING_SYSTEM_PROMPT},  # <-- FIXED!
    {
        "role": "user",
        "content": (
            f"USER QUERY: {query}\n\n"
            + "RELEVANT GRANT CONTEXT:\n"
            + context
            + "\n\nProvide a helpful response about these grants in plain markdown."
        ),
    },
]
```

AND remove the JSON parsing code (around line 560):

```python
# DELETE THIS ENTIRE BLOCK:
# Parse the full response to extract grant recommendations
try:
    data = json.loads(full_response)
    answer_markdown = data.get("answer_markdown", full_response)
    recommended_grant_ids = [g.get("grant_id") for g in data.get("recommended_grants", [])]
except Exception:
    answer_markdown = full_response
    recommended_grant_ids = []

# Just use the markdown directly:
answer_markdown = full_response
```

## COMPLETE REPLACEMENT CODE

Here's the complete fixed section for `/chat/stream`:

```python
# Generate streaming response
async def generate():
    try:
        # Step 1: Select top grants
        grants = select_top_grants(hits, query=query)

        # Step 2: Build context
        context = build_llm_context(query, hits, grants)

        # Step 3: Stream GPT response with MARKDOWN-ONLY prompt
        grants_list = "\n".join([f"- {g['title']} ({g['source']})" for g in grants[:5]])
        
        STREAMING_SYSTEM_PROMPT = f"""You are Ailsa, a UK research funding advisor.

Output PLAIN MARKDOWN (not JSON!). Be concise: 300-500 words.

Structure:
1. Brief summary (1-2 sentences)
2. Top 2-3 grants with why they fit
3. Next steps (2-3 actions)

The grants displayed to users:
{grants_list}

Focus on insights and strategy, not repeating metadata."""

        messages = [
            {"role": "system", "content": STREAMING_SYSTEM_PROMPT},
            {
                "role": "user",
                "content": f"USER QUERY: {query}\n\nGRANT CONTEXT:\n{context}\n\nProvide helpful advice in plain markdown.",
            },
        ]

        # Stream tokens
        full_response = ""
        for chunk in chat_llm_client.chat_stream(
            messages=messages,
            temperature=0.3,  # Lower for conciseness
            max_tokens=800,   # Reduced for brevity
        ):
            full_response += chunk
            yield f"data: {json.dumps({'type': 'token', 'content': chunk})}\n\n"

        # Send grants (use the ones we already selected)
        grant_refs = []
        for g in grants[:5]:
            grant_refs.append({
                "grant_id": g["grant_id"],
                "title": g["title"],
                "url": g["url"],
                "source": g["source"],
                "is_active": True,
                "total_fund_gbp": g.get("total_fund_gbp"),
                "closes_at": g.get("closes_at"),
                "score": g.get("best_score", 0.0)
            })

        yield f"data: {json.dumps({'type': 'grants', 'grants': grant_refs})}\n\n"
        yield f"data: {json.dumps({'type': 'done'})}\n\n"

    except Exception as e:
        logger.error(f"Streaming failed: {e}")
        yield f"data: {json.dumps({'type': 'token', 'content': 'Error generating response.'})}\n\n"
        yield f"data: {json.dumps({'type': 'done'})}\n\n"

return StreamingResponse(generate(), media_type="text/event-stream")
```

## SUMMARY

**Before:** GPT outputs JSON → gets streamed as `{"answer_markdown": "..."}` → shows as raw JSON in UI

**After:** GPT outputs markdown → gets streamed as plain text → displays beautifully in UI

The key insight: Streaming and JSON parsing don't mix well. Output markdown directly for streaming.
